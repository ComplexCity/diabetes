{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 1 Crawler algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before every crawl, make sure to remove the already visited addresses, then ...\n",
    "### 1. Extract all addresses to visit from a .gexf  \n",
    "### 2. For each address\n",
    "> - Create a new folder with name hashed \n",
    "> - Extract the main page as a .html and save it in the folder \n",
    "> - List all the hyperlinks contained in the page\n",
    "> > - For each sublinks, if it is a subpage of the main address (same domain), save it as .html in the folder with an incrementing number\n",
    "> - Once all html pages are saved, clean them to get the text so as to be parsed easily, extracting tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import networkx as nx\n",
    "import os\n",
    "import requests\n",
    "import pickle\n",
    "import hashlib\n",
    "import time\n",
    "import re\n",
    "from bs4 import BeautifulSoup as bs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .gexf\n",
    "G = nx.readwrite.gexf.read_gexf(\"diabetes-final-graph.gexf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of website found on the gexf file:  2355\n"
     ]
    }
   ],
   "source": [
    "address_tab = set()\n",
    "i=0\n",
    "for node in G.nodes():\n",
    "    # we check if there is a homepage set\n",
    "    homepage = G.node[node]['homepage']\n",
    "    if homepage != 'null':\n",
    "        #if yes\n",
    "        address_tab.add(homepage)\n",
    "    else:\n",
    "        #else we get the name\n",
    "        address_tab.add(G.node[node]['name'])\n",
    "\n",
    "print('Number of website found on the gexf file: ',len(address_tab)) #should be 2355 if files does not change   \n",
    "#print(address_tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At this point all the websites are in the var ADDRESS_TAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2  page(s) downloaded.\n",
      "Still 2353  page(s) to download out of  2355\n"
     ]
    }
   ],
   "source": [
    "# We don't want to re-download an existing folder\n",
    "# However, the name being hashed, we need to compare every name \n",
    "downloaded_websites = set()\n",
    "\n",
    "directories = os.listdir('pages/')\n",
    "for directory in directories:\n",
    "    downloaded_websites.add(directory) \n",
    "\n",
    "# Now we have all the already downloaded website BUT hashed    \n",
    "\n",
    "pending_websites = set(address_tab)    \n",
    "\n",
    "for d_site in downloaded_websites:\n",
    "    for p_site in pending_websites.copy():\n",
    "        if hashlib.md5(p_site.encode('utf-8')).hexdigest() == d_site:\n",
    "            #print(\"we remove \" + p_site)\n",
    "            #print(\"we remove \" + d_site)\n",
    "            pending_websites.remove(p_site)\n",
    "        \n",
    "# However the last found folder may not be fully downloaded\n",
    "\n",
    "\n",
    "\n",
    "#print(pending_website)\n",
    "print(len(downloaded_websites),' page(s) downloaded.')\n",
    "print('Still',len(pending_websites),' page(s) to download out of ',len(address_tab))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At this point :\n",
    "> - All the websites are in ADDRESS_TAB\n",
    "> - All the remaining website to download are in PENDING_WEBSITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We create folder  8a7619cfcd1d66c0c9b19b5b007d03a6 for the site  http://www.diabetespainhelp.com\n",
      "30 sublinks  found on this page\n",
      "#main-content\n",
      "/\n",
      "/\n",
      "/step-on-up-to-diabetic-nerve-pain\n",
      "/about-diabetic-nerve-pain\n",
      "/importance-of-taking-action\n",
      "/diabetic-nerve-pain-assessment\n",
      "/talk-to-your-doctor\n",
      "http://www.lyrica.com/DPN/Introduction\n",
      "/about-diabetic-nerve-pain\n",
      "/importance-of-taking-action\n",
      "/talk-to-your-doctor\n",
      "/step-on-up-to-diabetic-nerve-pain\n",
      "/importance-of-taking-action\n",
      "/talk-to-your-doctor\n",
      "#\n",
      "/\n",
      "/step-on-up-to-diabetic-nerve-pain\n",
      "/about-diabetic-nerve-pain\n",
      "/importance-of-taking-action\n",
      "/diabetic-nerve-pain-assessment\n",
      "/talk-to-your-doctor\n",
      "http://www.lyrica.com/DPN/Introduction\n",
      "/site-map\n",
      "http://www.pfizer.com/general/terms\n",
      "http://www.pfizer.com/general/privacy\n",
      "http://www.pfizer.com/about\n",
      "/\n",
      "http://www.pfizer.com\n",
      "http://www.omniture.com\n",
      "0 sublinks of the same domain found on this page\n"
     ]
    }
   ],
   "source": [
    "for website in pending_websites:\n",
    "    #Create the folder with name hashed - to avoid the special characters\n",
    "    print('We create folder ', hashlib.md5(website.encode('utf-8')).hexdigest(), 'for the site ', website)\n",
    "    folder_name = hashlib.md5(website.encode('utf-8')).hexdigest()\n",
    "    os.mkdir(\"pages/\" + folder_name)    \n",
    "    \n",
    "    \n",
    "    #go to the page, get the html and save it\n",
    "    url =''\n",
    "    #if we don't have the website page, we need to add the http adress (note that the 's' of https seems auto added)\n",
    "    if website[:4]!='http':\n",
    "        url+='http://'\n",
    "    url+=website\n",
    "    p = requests.get(url)\n",
    "    with open(\"pages/\"+ folder_name +'/index.html','w+', encoding='utf-8') as fp:\n",
    "        fp.write(p.text)\n",
    "            \n",
    "    #with all hyperlinks, if they are subpage, go to them and save the html \n",
    "    soup = bs(p.text)\n",
    "    sublinks = set()   \n",
    "    print(str(len(soup.find_all('a'))) + ' sublinks  found on this page')    \n",
    "    for link in soup.find_all('a'):\n",
    "        try:\n",
    "            print(link['href'])\n",
    "            if website in link['href'] or website.lower() in link['href']:\n",
    "                sublinks.add(link['href'])\n",
    "        except KeyError:\n",
    "            #no href in this link\n",
    "            continue\n",
    "    print(str(len(sublinks)) + ' sublinks of the same domain found on this page')\n",
    "    \n",
    "    page_cpt=1 \n",
    "    \n",
    "    for subpage in sublinks:\n",
    "        print('Treating page ',page_cpt,' out of ',len(sublinks),' please wait ...')\n",
    "        sub_p = requests.get(subpage)\n",
    "        sub_soup = bs(sub_p.text,'html.parser')\n",
    "        #print(sub_soup.get_text())\n",
    "        #clean the page\n",
    "        #soup = bs(p.text, 'html.parser')\n",
    "        #to_clean = soup.findAll(text=True)\n",
    "        #print(to_clean)\n",
    "        #regex = [r\"\\/\\*(.*)\\*\\/\",\"\\_(.*)\\;\"]\n",
    "        #subst = \"  \"\n",
    "        #for rex in regex:\n",
    "        #    result = to_clean\n",
    "        #    result = re.sub(rex, subst, to_clean, 0, re.MULTILINE)\n",
    "        #    to_clean = result\n",
    "        \n",
    "        with open(\"pages/\"+ folder_name+\"/\" + str(page_cpt)+ \".txt\",'w+', encoding='utf-8') as fp_sub:\n",
    "            fp_sub.write(sub_soup.get_text())     \n",
    "        #only work on the 5 first pages\n",
    "        if page_cpt == 1:\n",
    "            break\n",
    "        page_cpt+=1\n",
    "        #end of the subpage scrapping\n",
    "        time.sleep(2) \n",
    "    #end of the main website scrapping    \n",
    "    time.sleep(2)    \n",
    "    break #test only on the first one\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### === SANDBOX ===\n",
    "\n",
    "website = 'Asweetlife.org'\n",
    "    #folder_name = website\n",
    "    #os.mkdir(\"pages/\" + folder_name)\n",
    "    #pickle for hash md5\n",
    "    \n",
    "url =''\n",
    "    #if we don't have the website page, we need to add the http adress (nte that the 's' of https seems auto added)\n",
    "if website[:3]!='http':\n",
    "    url+='http://'\n",
    "url+=website\n",
    "    \n",
    "    #go to the page, get the html and save it\n",
    "p = requests.get(url)\n",
    "with open(\"pages/\"+ folder_name +\"/index.html\",'w+', encoding='utf-8') as fp:\n",
    "    fp.write(p.text)\n",
    "        \n",
    "    #with all hyperlinks, if they are subpage, go to them and save the html\n",
    "soup = bs(p.text)\n",
    "links = soup.find_all('a')\n",
    "sublinks = set()    \n",
    "for link in links:\n",
    "    if website in link['href'] or website.lower() in link['href']:\n",
    "        sublinks.add(link['href'])            \n",
    "print(str(len(sublinks)) + ' sublinks of the same domain found on this page')\n",
    "page_cpt=1\n",
    "for subpage in sublinks:\n",
    "    print('Treating page ',page_cpt,' out of ',len(sublinks),' please wait ...')\n",
    "    p = requests.get(subpage)\n",
    "    #clean the page\n",
    "    \n",
    "    #save it\n",
    "    with open(\"pages/\"+ folder_name +\"/\" + str(page_cpt)+ \".html\",'w+', encoding='utf-8') as fp:\n",
    "        fp.write(p.text)\n",
    "    #only work on the 5 first pages\n",
    "    if page_cpt == 5:\n",
    "        break\n",
    "    page_cpt+=1    \n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
