{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 1 Crawler algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before every crawl, make sure to remove the already visited addresses, then ...\n",
    "### 1. Extract all addresses to visit from a .gexf  \n",
    "### 2. For each address\n",
    "> - Create a new folder with name hashed \n",
    "> - Extract the main page as a .html and save it in the folder \n",
    "> - List all the hyperlinks contained in the page\n",
    "> > - For each sublinks, if it is a subpage of the main address (same domain), save it as .html in the folder with an incrementing number\n",
    "> - Once all html pages are saved, clean them to get the text so as to be parsed easily, extracting tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import networkx as nx\n",
    "import os\n",
    "import requests\n",
    "import pickle\n",
    "import hashlib\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .gexf\n",
    "G = nx.readwrite.gexf.read_gexf(\"diabetes-final-graph.gexf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of website found on the gexf file:  2355\n"
     ]
    }
   ],
   "source": [
    "address_tab = set()\n",
    "i=0\n",
    "for node in G.nodes():\n",
    "    # we check if there is a homepage set\n",
    "    homepage = G.node[node]['homepage']\n",
    "    if homepage != 'null':\n",
    "        #if yes\n",
    "        address_tab.add(homepage)\n",
    "    else:\n",
    "        #else we get the name\n",
    "        address_tab.add(G.node[node]['name'])\n",
    "\n",
    "print('Number of website found on the gexf file: ',len(address_tab)) #should be 2355 if files does not change   \n",
    "#print(address_tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At this point all the websites are in the var ADDRESS_TAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  page(s) downloaded.\n",
      "Still 2354  page(s) to download out of  2355\n"
     ]
    }
   ],
   "source": [
    "# We don't want to re-download an existing folder\n",
    "# However, the name being hashed, we need to compare every name \n",
    "downloaded_sites = set()\n",
    "\n",
    "directories = os.listdir('pages/')\n",
    "for directory in directories:\n",
    "    downloaded_sites.add(directory) \n",
    "\n",
    "# Now we have all the already downloaded website BUT hashed    \n",
    "\n",
    "pending_sites = set(address_tab)    \n",
    "\n",
    "for d_site in downloaded_sites:\n",
    "    for p_site in pending_sites.copy():\n",
    "        if hashlib.md5(p_site.encode('utf-8')).hexdigest() == d_site:\n",
    "            pending_sites.remove(p_site)\n",
    "        \n",
    "# However the last found folder may not be fully downloaded\n",
    "\n",
    "\n",
    "\n",
    "#print(pending_website)\n",
    "print(len(downloaded_websites),' page(s) downloaded.')\n",
    "print('Still',len(pending_websites),' page(s) to download out of ',len(address_tab))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At this point :\n",
    "> - All the websites are in ADDRESS_TAB\n",
    "> - All the remaining website to download are in PENDING_WEBSITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We create folder  81e10320712163a7b0a1abf95263c642 for the site  idodiabetes.Wordpress.com\n"
     ]
    }
   ],
   "source": [
    "for site in pending_website:\n",
    "    #Create the folder with name hashed - to avoid the special characters\n",
    "    print('We create folder ', hashlib.md5(site.encode('utf-8')).hexdigest(), 'for the site ', site)\n",
    "    folder_name = hashlib.md5(site.encode('utf-8')).hexdigest()\n",
    "    os.mkdir(\"pages/\" + folder_name)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #pickle for hash md5\n",
    "    \n",
    "    \n",
    "    \n",
    "    #go to the page, get the html and save it\n",
    "    url =''\n",
    "    #if we don't have the website page, we need to add the http adress (nte that the 's' of https seems auto added)\n",
    "    if website[:3]!='http':\n",
    "        url+='http://'\n",
    "    url+=website\n",
    "    p = requests.get(url)\n",
    "    with open(\"pages/\"+ folder_name +'/index.html','w+', encoding='utf-8') as fp:\n",
    "        fp.write(p.text)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    #with all hyperlinks, if they are subpage, go to them and save the html \n",
    "    soup = bs(p.text)\n",
    "    links = soup.find_all('a')          \n",
    "              \n",
    "              \n",
    "              \n",
    "              \n",
    "              \n",
    "              \n",
    "              \n",
    "    #parse every html, clean them and have a .txt file \n",
    "    \n",
    "    break #test only on the first one\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 sublinks of the same domain found on this page\n",
      "Treating page  1  out of  200  please wait ...\n",
      "Treating page  2  out of  200  please wait ...\n",
      "Treating page  3  out of  200  please wait ...\n",
      "Treating page  4  out of  200  please wait ...\n"
     ]
    }
   ],
   "source": [
    "website = 'Asweetlife.org'\n",
    "    #folder_name = website\n",
    "    #os.mkdir(\"pages/\" + folder_name)\n",
    "    #pickle for hash md5\n",
    "    \n",
    "url =''\n",
    "    #if we don't have the website page, we need to add the http adress (nte that the 's' of https seems auto added)\n",
    "if website[:3]!='http':\n",
    "    url+='http://'\n",
    "url+=website\n",
    "    \n",
    "    #go to the page, get the html and save it\n",
    "p = requests.get(url)\n",
    "with open(\"pages/\"+ folder_name +\"/index.html\",'w+', encoding='utf-8') as fp:\n",
    "    fp.write(p.text)\n",
    "        \n",
    "    #with all hyperlinks, if they are subpage, go to them and save the html\n",
    "soup = bs(p.text)\n",
    "links = soup.find_all('a')\n",
    "sublinks = set()    \n",
    "for link in links:\n",
    "    if website in link['href'] or website.lower() in link['href']:\n",
    "        sublinks.add(link['href'])            \n",
    "print(str(len(sublinks)) + ' sublinks of the same domain found on this page')\n",
    "page_cpt=1\n",
    "for subpage in sublinks:\n",
    "    print('Treating page ',page_cpt,' out of ',len(sublinks),' please wait ...')\n",
    "    p = requests.get(subpage)\n",
    "    #clean the page\n",
    "    \n",
    "    #save it\n",
    "    with open(\"pages/\"+ folder_name +\"/\" + str(page_cpt)+ \".html\",'w+', encoding='utf-8') as fp:\n",
    "        fp.write(p.text)\n",
    "    #only work on the 5 first pages\n",
    "    if page_cpt == 5:\n",
    "        break\n",
    "    page_cpt+=1    \n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
